NLP Note
http://ruder.io/deep-learning-nlp-best-practices/

词向量：
词向量的最优维度取决于所面对的问题。通常，小维度的词向量在诸如命名实体识别(named entity recognition)和词性标注(POS tagging)这样一些基于句子语法的任务中取得较好的效果。大维度的词向量更适合像语义分析这种任务。

深度：
NLP任务中的模型，浅层的网络（1-2层）已经足够好了。
深层网络在处理单字符任务时表现良好。

连接层：
Highway layers，       激活函数为  h=t⊙g(Wx+b)+(1−t)⊙x
Residual connections， 激活函数为  h=g(Wx+b)+x
Dense connections,     激活函数为  hl=g(W[x1;…;xl]+b)

Dropout:
普通的dropout在NLP任务中并不适用
针对循环神经网络，提出了recurrent dropout，它固定了每一层中的置0的神经元，不至于让多层dropout将输出全置为0

多任务学习:
Auxiliary objectives：
Task-specific layers：

注意力机制:

优化：
Adam，足够好的优化器；SGD经过调参，也可以超过Adam
Adam with 2 restarts and learning rate annealing is faster and performs better than SGD with annealing.
进行2次重启和学习率退火的Adam算法比使用了退火的SGD更快更优

集成：
ensemble方法被证明是一种非常有效的方法，但在神经网络中的应用代价有些高昂
将一个模型不同checkpoint的效果进行集成被证明是有效的；但如果条件允许，用多个独立训练的模型来集成可能会更好。

LSTM tricks:
learning the initial state：学习初始状态而不是固定住
typing input and output embeddings：输入和输出词向量共享参数，针对数据量少的情况特别有用
gradient norm clipping：clipping the global norm of the gradient yields more significant improvements
down-projection

Task specific best pratices
文本分类：
cnn filters: 过滤核大小为3,4,5提供了最好的效果，feature maps 的范围为50-600
聚合函数：1-max-pooling比平均池化和k-max池化要好

sequence labelling:
tagging scheme:    BIO...
CRF output layer:  如果输出层存在依赖，使用linear-chain CRF可以提高效果
constrained decoding:

自然语言生成
模型覆盖范围(coverage)：注意力机制可以捕获重复输出的情况

神经机器翻译
embedding dimensionality: 词向量的难度为2048时表现最好，但只是小幅胜出，128有时也惊人地不错。
encoder and decoder depth: encoder的深度不需要超过2-4层，decoder也不需要超过四层
directionality：两向的encoders比单一方向的要好一点点，在单方向encoders中，对序列进行反转能取得更好的效果
sub-word translation：有效地减少了out-of-vocabulary-word的数量，效果比full-word更好些

