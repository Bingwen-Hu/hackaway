==================== cross validation ====================
leave one out cv: leave one obervation and using the rest to train the model 
repeats n times. It has lower bias and higher variance and compute costly

k-fold cv: split the dataset into k-fold and using k-1 fold to train the model
and using one fold to test it. k is chosed 5 or 10 typically. K-fold cv has
lower variance but higher bias, it computes less.

==================== bootstrap ====================
bootstrap is a powerful method when the number of samples is very limited. 
Every time it take n obervations from the original dataset. The obervations
can repeat. This procedure repeat B times where B is a very large number.
Finally we can use the average of this B times datasets to estimate the
original dataset.
